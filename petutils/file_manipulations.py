from ast import Raise
import math
import re
import os
import json
import argparse
from bids import BIDSLayout, BIDSValidator
from bids.exceptions import BIDSValidationError
from glob import glob
from pathlib import Path
from nilearn.image import concat_imgs
from collections import defaultdict
from typing import Literal
from numpy import rec



validator = BIDSValidator()

def collect_runs(bids_dataset:str):
    bids_dataset = Path(bids_dataset)
    try:
        layout = BIDSLayout(bids_dataset.absolute().__str__())
        everything = layout.get()
        run_list = []
        for item in everything:
            if item.get_entities().get('run', None):
                run_list.append(Path(item.path))
    except BIDSValidationError:
        # collect files that contain run
        run_list = glob(f'{bids_dataset.absolute().__str__()}{os.sep}**{os.sep}*_run-*', recursive=True)
        run_list = [Path(run) for run in run_list]
    
    grouped_runs = defaultdict(list)
    for run in run_list:
        alike = re.sub(r'_run-\d+', '_run-XX', str(run))
        grouped_runs[alike].append(run)
    
    # sort runs
    for key in grouped_runs.keys():
        grouped_runs[key].sort()

    return dict(grouped_runs)


def organize_by_extensions(grouped_runs: dict):
    """
    accepts a set of grouped runs previously generated by collect runs.
    organinizes a set of runs by file extension for nii, tsv, and json.

    :param grouped_runs: all BIDS files containing `run` in the filename.
    :type grouped_runs: dict
    """
    extensions = ['tsv', 'json', 'nii']
    if not grouped_runs:
        raise Exception(f"Must provide grouped_run")

    grouped_by_run_and_extension = defaultdict(dict)
    for key in grouped_runs.keys():
        for f in grouped_runs[key]:
            match f:
                case 'tsv':
                    grouped_by_run_and_extension[k] 
        
        
        
def concat_sidecars(sidecar_jsons:list, set_time_zero=False):
    set_time_zero = set_time_zero
    storage = {}
    # sort sidecar jsons
    sidecar_jsons.sort()

    # load each sidecar
    for s in sidecar_jsons:
        with open(s, 'r') as infile:
            storage[s] = json.load(infile)
    # verify timezero is the same for each sidecar json
    time_zero_set = set()
    for s, values in storage.items():
        time_zero_set.add((values.get("TimeZero")))
    
    if len(time_zero_set) > 1 and not set_time_zero:
        raise Exception(f"Multiple TimeZero Values found for {sidecar_jsons.keys()}\nCall with set_time_zero=True to set TimeZero relative to first run.")
    

def concat_niftis(niftis:list, remove_original=False):
    # sort nifti list
    one_from_many = niftis
    one_from_many.sort()
    one = concat_imgs(one_from_many)
    # remove run from filename
    new_file_name = re.sub(r'_run-\d+', '', str(niftis[0]))
    # save output
    one.to_filename(new_file_name)

    if remove_original and Path(new_file_name).exists():
        [os.remove(str(orig)) for orig in niftis]
    else:
        Raise(f"Failed to save {new_file_name}, keeping original images {niftis}")

    return new_file_name

def concat_niftis_cli():
    parser = argparse.ArgumentParser()
    parser.add_argument("niftis", nargs="*")
    parser.add_argument("-r", "--remove-original", help="Clean up old runs of nifti's, default behavior is to keep them.",
    action="store_true", default=False)
    args = parser.parse_args()
    concat_niftis(**args)


